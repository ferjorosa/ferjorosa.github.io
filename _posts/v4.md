<h2 id="decision-trees-strengths-limitations">Strengths and Limitations of Decision Trees</h2>

Decision trees provide a straightforward way to model decisions under uncertainty. Each path from start to finish shows a sequence of choices and events that lead to a specific outcome. The left-to-right layout makes the timing of decisions and chance events visually clear, helping to avoid confusion about what is known at each point. 

For small problems with only a few stages, decision trees can be evaluated using basic arithmetic. This simplicity makes them accessible to a wide audience, including those without technical training. They serve as effective tools for teaching, storytelling, and supporting real-world decisions. 

Yet this intuitive structure comes with notable drawbacks. One issue is the risk of combinatorial explosion. Another limitation is that decision trees do not explicitly show conditional independencies.

<h3 id="combinatorial-explosion">Combinatorial Explosion</h3>
The memory required to store a decision tree and the time required to process it both **increase
exponentially** with the number of variables and their possible states, whether they are decisions or
probabilistic outcomes. In a symmetric problem with $$n$$ variables, each having $$k$$ possible outcomes, you face $$k^{n}$$ distinct paths. Since a decision tree represents all scenarios explicitly, a problem with 50 binary variables would yield an impractical $$2^{50}$$ paths (<a href="https://pshenoy.ku.edu/Papers/EOLSS09.pdf"><u>Shenoy, 2009</u></a>).

The number of decision paths is profoundly affected by the order and meaning of the variables (i.e., the problem's definition). In our original oil field investment problem from <a href="https://ferjorosa.github.io/blog/2025/06/08/decision-theory-I.html"><u>Part I</u></a>, the options <span style="color:red;">Do not perform test</span> and <span style="color:red;">Do not buy</span> prune the tree, resulting in 12 distinct decision paths. This is an *asymmetric* problem structure:

<center>
<table>
  <tr>
    <td align="center">
      <img src="/assets/2025-06-14-decision-theory-II/oil_asymmetric_tree_annotated.png" alt="Decision tree diagram of the asymmetric oil problem from Part I" height="240">
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <i><b>Figure 1.</b> Decision tree diagram of the asymmetric oil field investment problem from Part I.</i>
    </td>
  </tr>
</table>
</center>

Conversely, a problem with the same types of variables, but structured *symmetrically*, would yield significantly more paths. For instance, imagine the company must always choose to either <span style="color:red;">Invest in Field A</span> or <span style="color:red;">Invest in Field B</span>. Both fields then undergo a geological test (say, <span style="color:red;">Test X</span> or <span style="color:red;">Test Y</span>, each with different costs and accuracy profiles), followed by the actual drilling revealing the field's quality. This structure forces every path to be fully explored, doubling the number of distinct paths from 12 to 24:

<center>
<table>
  <tr>
    <td align="center">
      <img src="/assets/2025-06-14-decision-theory-II/oil_symmetric_tree_annotated.png" alt="Decision tree diagram of a hypothetical symmetric oil problem" height="240">
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <i><b>Figure 2.</b> Decision tree diagram of a hypothetical symmetric oil field investment problem.</i>
    </td>
  </tr>
</table>
</center>

Even this revised example demonstrates how swiftly a decision tree can escalate beyond practical use. For instance, merely replacing the three-level oil quality (high/medium/low) with a more granular five-level scale (excellent/good/average/poor/dry) would push the total from 24 to 40 terminal nodes. This occurs without even considering longer time horizons, dynamic market-price scenarios, or additional complex choices.

This **combinatorial explosion** not only affects computational tractability but, even at more modest levels, severely compromises interpretability. As a rule of thumb, once a tree approaches about 100 terminal nodes, it loses its key strength: easy readability and intuitive understanding.


<h3 id="hidden-independence">Hidden Conditional Independencies</h3>

In addition to the issue of combinatorial explosion, decision trees have another important limitation: they assume a strict, linear chain of dependence. In a decision tree, every variable is implicitly conditioned on *all* previous events along its particular path. This rigid structure prevents us from explicitly representing one of the most important concepts in probabilistic modeling: <b><a href="https://en.wikipedia.org/wiki/Conditional_independence"><u>conditional independence</u></a></b>.

To illustrate this, consider a generic problem with four <span style="color:purple;"><b>random variables</b></span>: $$A$$ and $$B$$ (each with two possible states, $$a_1, a_2$$ and $$b_1, b_2$$, respectively), and $$C$$ and $$D$$ (each with three possible states, $$c_1, c_2, c_3$$ and $$d_1, d_2, d_3$$). In a traditional decision tree, where conditional independencies cannot be explicitly represented, you must specify the entire joint probability distribution for all variables. This means assigning a probability to every possible combination of outcomes, as shown in the joint probability table below:

<details style="margin: 1em 0; padding: 0.5em; border: 1px solid #ddd; border-radius: 4px;">

<summary style="cursor: pointer; font-weight: bold; padding: 0.5em;">P(A, B, C, D)</summary>

<table>
  <thead>
    <tr>
      <th>$$A$$</th>
      <th>$$B$$</th>
      <th>$$C$$</th>
      <th>$$D$$</th>
      <th style="text-align: center;">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{1},b_{1},c_{1},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{1},b_{1},c_{1},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{1},b_{1},c_{1},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{1},b_{1},c_{2},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{1},b_{1},c_{2},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{1},b_{1},c_{2},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{1},b_{1},c_{3},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{1},b_{1},c_{3},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{1},b_{1},c_{3},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{1},b_{2},c_{1},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{1},b_{2},c_{1},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{1},b_{2},c_{1},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{1},b_{2},c_{2},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{1},b_{2},c_{2},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{1},b_{2},c_{2},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{1},b_{2},c_{3},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{1},b_{2},c_{3},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{1},b_{2},c_{3},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{2},b_{1},c_{1},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{2},b_{1},c_{1},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{2},b_{1},c_{1},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{2},b_{1},c_{2},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{2},b_{1},c_{2},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{2},b_{1},c_{2},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{2},b_{1},c_{3},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{2},b_{1},c_{3},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{2},b_{1},c_{3},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{2},b_{2},c_{1},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{2},b_{2},c_{1},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{2},b_{2},c_{1},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{2},b_{2},c_{2},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{2},b_{2},c_{2},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{2},b_{2},c_{2},d_{3})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(a_{2},b_{2},c_{3},d_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(a_{2},b_{2},c_{3},d_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(a_{2},b_{2},c_{3},d_{3})$$</td>
    </tr>
  </tbody>
</table>

</details>

**Total distinct probabilities:** $$ 2 \; (\text{for } A) \times 2 \; (\text{for } B) \times 3 \; (\text{for } C) \times 3 \; (\text{for } D) = \mathbf{36} $$.

<div style="background-color: #e0f7fa; padding: 10px; border-radius: 5px;">
  This example demonstrates conditional independencies using a simplified version of a decision tree, where all nodes are probabilistic (often called a <a href="https://en.wikipedia.org/wiki/Tree_diagram_(probability_theory)"><u>probability tree</u></a>). However, the core concepts and benefits of explicit representation apply equally to the chance nodes within any general decision tree.
</div>
<br>

Now, let's say that conditional independencies do exist in this problem. For instance, let's say that $$B$$ is conditionally independent of $$C$$ and $$D$$ given $$A$$. We denote that statement by $$(B \bot \{C, D\} \mid A)$$. In that case:

$$
P(B \mid A, C, D) = P(B \mid A)
$$

This fundamental concept allows us to represent relationships far more efficiently. Consider Figure 3, which implies the following conditional independence statements:
* &nbsp;$$ (B \bot \{C, D\} \mid A)$$
* &nbsp;$$ (C \bot B \mid A)$$
* &nbsp;$$ (D \bot \{A,B\} \mid C)$$

The diagram corresponds to the directed acyclic graph of a <a href="https://en.wikipedia.org/wiki/Bayesian_network"><u>Bayesian network</u></a>, which visually encodes these independencies: arrows indicate direct probabilistic influence, while the absence of an arrow between two nodes reflects a conditional independence given their parents.

<center>
<table>
  <tr>
    <td align="center">
      <img src="/assets/2025-06-14-decision-theory-II/bayesian_network_example.png" alt="Decision tree diagram of a hypothetical symmetric oil problem" height="300">
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <i><b>Figure 3.</b> Bayesian network illustrating conditional independencies among A, B, C, and D.</i>
    </td>
  </tr>
</table>
</center>

When these conditional independencies are recognized and modeled, we no longer need to construct a single large joint probability table. Instead, the full joint probability distribution can be factored into a product of smaller, more manageable conditional probability tables. In our example, this means we only need to specify:

* &nbsp;$$ P(A) \rightarrow 2 = 2 $$ entries
* &nbsp;$$ P(B \mid A) \rightarrow 2 \cdot 2 = 4 $$ entries  
* &nbsp;$$ P(C \mid A) \rightarrow 2 \cdot 3 = 6 $$ entries
* &nbsp;$$ P(D \mid C) \rightarrow 3 \cdot 3 = 9 $$ entries

<details style="margin: 1em 0; padding: 0.5em; border: 1px solid #ddd; border-radius: 4px;">

<summary style="cursor: pointer; font-weight: bold; padding: 0.5em;">P(A)</summary>

<table>
  <thead>
    <tr>
      <th>$$A$$</th>
      <th style="text-align: center;">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$P(a_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$P(a_{2})$$</td>
    </tr>
  </tbody>
</table>

</details>


<details style="margin: 1em 0; padding: 0.5em; border: 1px solid #ddd; border-radius: 4px;">

<summary style="cursor: pointer; font-weight: bold; padding: 0.5em;">P(B | A)</summary>

<table>
  <thead>
    <tr>
      <th>$$A$$</th>
      <th>$$B$$</th>
      <th style="text-align: center;">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$P(b_{1} \mid a_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$P(b_{2} \mid a_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{1}$$</td>
      <td>$$P(b_{1} \mid a_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$b_{2}$$</td>
      <td>$$P(b_{2} \mid a_{2})$$</td>
    </tr>
  </tbody>
</table>

</details>

<details style="margin: 1em 0; padding: 0.5em; border: 1px solid #ddd; border-radius: 4px;">

<summary style="cursor: pointer; font-weight: bold; padding: 0.5em;">P(C | A)</summary>

<table>
  <thead>
    <tr>
      <th>$$A$$</th>
      <th>$$C$$</th>
      <th style="text-align: center;">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$P(c_{1} \mid a_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$P(c_{2} \mid a_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{1}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$P(c_{3} \mid a_{1})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$c_{1}$$</td>
      <td>$$P(c_{1} \mid a_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$c_{2}$$</td>
      <td>$$P(c_{2} \mid a_{2})$$</td>
    </tr>
    <tr>
      <td>$$a_{2}$$</td>
      <td>$$c_{3}$$</td>
      <td>$$P(c_{3} \mid a_{2})$$</td>
    </tr>
  </tbody>
</table>

</details>

<details style="margin: 1em 0; padding: 0.5em; border: 1px solid #ddd; border-radius: 4px;">

<summary style="cursor: pointer; font-weight: bold; padding: 0.5em;">P(D | C)</summary>

<table>
  <thead>
    <tr>
      <th>$$C$$</th>
      <th>$$D$$</th>
      <th style="text-align: center;">Probability</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$$c_{1}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(d_{1} \mid c_{1})$$</td>
    </tr>
    <tr>
      <td>$$c_{1}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(d_{2} \mid c_{1})$$</td>
    </tr>
    <tr>
      <td>$$c_{1}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(d_{3} \mid c_{1})$$</td>
    </tr>
    <tr>
      <td>$$c_{2}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(d_{1} \mid c_{2})$$</td>
    </tr>
    <tr>
      <td>$$c_{2}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(d_{2} \mid c_{2})$$</td>
    </tr>
    <tr>
      <td>$$c_{2}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(d_{3} \mid c_{2})$$</td>
    </tr>
    <tr>
      <td>$$c_{3}$$</td>
      <td>$$d_{1}$$</td>
      <td>$$P(d_{1} \mid c_{3})$$</td>
    </tr>
    <tr>
      <td>$$c_{3}$$</td>
      <td>$$d_{2}$$</td>
      <td>$$P(d_{2} \mid c_{3})$$</td>
    </tr>
    <tr>
      <td>$$c_{3}$$</td>
      <td>$$d_{3}$$</td>
      <td>$$P(d_{3} \mid c_{3})$$</td>
    </tr>
  </tbody>
</table>

</details>

**Total distinct probabilities:** $$2 + 4 + 6 + 9 = \mathbf{21}$$.

This represents a **42% decrease** from the 36 entries required in the full joint probability table. By reducing the number of parameters that must be specified, the model becomes much more manageable, transparent, and easier to modify. As decision problems increase in complexity, the advantages of explicitly modeling conditional independencies grow even more significant.

<h2 id="decision_networks">Influence Diagrams</h2>

Building on the foundation of Bayesian networks, **influence diagrams** (<a href=""><u>Howard & Matheson, 1984</u></a>), also known as **decision network**, provide a powerful extension that seamlessly integrates decision-making into probabilistic models. Unlike decision trees, influence diagrams avoid combinatorial explosion by factorizing the joint probability distribution and naturally express conditional independencies through their graphical structure.

An influence diagram enhances a Bayesian network by adding two types of nodes:

* <span style="color:red;"><b>Decision nodes</b></span>: Shown as squares, these represent points where the decision-maker chooses among available actions.
* <span style="color:blue;"><b>Outcome nodes</b></span>: Illustrated as diamonds, these indicate the resulting utilities or values associated with different decision paths.

<span style="color:purple;"><b>Chance nodes</b></span> (circles) function as in Bayesian networks, often reusing the same conditional probability tables. In influence diagrams, arcs serve two main purposes: **informational arcs** (into decision nodes) indicate what information is available when a choice is made, while **conditional arcs** (into chance or utility nodes) represent probabilistic or functional dependencies on parent variables, showing which factors affect outcomes or payoffs—without implying causality or temporal order.


<h2 id="modelling_oil_problem">Modelling the Oil Problem</h2>

Modeling takes place at multiple levels. Similar to constructing a decision tree, drawing the influence diagram represents the qualitative description of the problem (structural or graphical level). Next, quantitative information is incorporated (numerical level) to fully specify the model.

One important caveat of influence diagrams is that they only work with symmetric problems. In the case of an asymmetric problem, techniques are used to convert it into a symmetric one. This leads to an increase in the size of the problem, since artificial states (sometimes unintuitive) are usually defined, and as a result, the computational burden increases.

In large and highly asymmetric problems, it is not so straightforward to use these kinds of resources (artificial alternatives and states, degenerate probabilities and utilities) to convert a problem into an equivalent symmetric one. For more information on this topic, see <a href="https://cig.fi.upm.es/wp-content/uploads/2024/01/A-Comparison-of-Graphical-Techniques-for-Asymmetric-Decision-Problems.pdf"><u>Bielza & Shenoy (1999)</u></a>.

In this article, we will use a standard influence diagram. To maintain problem symmetry, an extra state <span style="color:purple;">no results</span> should be added to the test results (<span style="color:purple;"><b>R</b></span>) variable . This is because results are only observed if the test is performed.

<h3 id="modelling_oil_problem_qualitative">Modelling Qualitative Information</h3>

The following image displays the influence diagram structure for the oil problem:

<center>
<table>
  <tr>
    <td align="center">
      <img src="/assets/2025-06-14-decision-theory-II/decision_network_oil.png" alt="Influence diagram structure of the asymmetric oil problem from Part I" height="175">
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <i><b>Figure 1a.</b> Traditional influence diagram for the oil problem. Informational arcs arcs</i>
    </td>
  </tr>
</table>
</center>

This diagram illustrates a traditional influence diagram, which operates under the assumption of perfect recall. The information arcs indicate that the Test / No Test (<span style="color:red;"><b>T</b></span>) decision is made prior to the Buy / No Buy (<span style="color:red;"><b>B</b></span>) decision. Furthermore, no information is available before making the test decision, and the test results are known when making the buy decision. This establishes the temporal sequence of variables: <span style="color:red;"><b>T</b></span>, <span style="color:purple;"><b>R</b></span>, <span style="color:red;"><b>B</b></span>, and finally <span style="color:purple;"><b>Q</b></span> (oil field quality).

However, traditional influence diagrams can become computationally complex to solve, particularly for intricate problems, and they may not accurately reflect the realistic limitations of human decision-making. For these reasons, LIMIDs (<a href="https://web.math.ku.dk/~lauritzen/papers/limids.pdf">Lauritzen & Nilsson, 2001)</a> are often the preferred choice. These models relax the perfect recall assumption and allow for explicit representation of limited memory. Memory arcs in LIMIDs explicitly specify which past decisions and observations are remembered and used for each current decision.

The subsequent image presents the corresponding LIMID, enhanced with a green memory arc. This memory arc extends from <span style="color:red;"><b>T</b></span> to <span style="color:red;"><b>B</b></span> because the outcome of the porosity test decision is crucial for the subsequent decision on buying or not buying the field.

<center>
<table>
  <tr>
    <td align="center">
      <img src="/assets/2025-06-14-decision-theory-II/decision_network_oil_limid.png" alt="Influence diagram structure of the asymmetric oil problem from Part I" height="175">
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <i><b>Figure 1b.</b> LIMID for the oil problem with a memory arc shown in green.</i>
    </td>
  </tr>
</table>
</center>

For this problem, we will utilize the LIMID version of the oil decision problem.

<h3 id="modelling_oil_problem_quantitative">Modelling Quantitative Information</h3>

Since all our random variables are categorical, we can conveniently represent the quantitative information of the model in tabular form. Below, we specify the prior probabilities, the conditional probability tables, and the utility values relevant to the oil field decision problem.

Prior probabilities for the oil field quality (<b><span style="color: purple;">Q</span></b>):

<table>
  <tr>
    <th colspan="2" style="text-align: center;">P(<span style="color: purple"><b>Q</b></span>)</th>
  </tr>
  <tr>
    <td><span style="color: purple;">high</span></td>
    <td>0.35</td>
  </tr>
  <tr>
    <td><span style="color: purple;">medium</span></td>
    <td>0.45</td>
  </tr>
  <tr>
    <td><span style="color: purple;">low</span></td>
    <td>0.2</td>
  </tr>
</table>

Conditional probabilities of observing each possible test result (<b><span style="color: purple;">R</span></b>), given the true oil field quality (<b><span style="color: purple;">Q</span></b>) and whether the test was performed (<b><span style="color: red;">T</span></b>):

<table>
  <tr>
    <th rowspan="2" style="text-align: center;">P(<span style="color: purple">R</span> | <span style="color: purple">Q</span>, <span style="color: red">T</span>)</th>
    <th colspan="3" style="text-align: center;"><span style="color: red;">Perform test</span></th>
    <th colspan="3" style="text-align: center;"><span style="color: red;">Do not perform test</span></th>
  </tr>
  <tr>
    <th style="text-align: center;"><span style="color: purple;">high</span></th>
    <th style="text-align: center;"><span style="color: purple;">medium</span></th>
    <th style="text-align: center;"><span style="color: purple;">low</span></th>
    <th style="text-align: center;"><span style="color: purple;">high</span></th>
    <th style="text-align: center;"><span style="color: purple;">medium</span></th>
    <th style="text-align: center;"><span style="color: purple;">low</span></th>
  </tr>
  <tr>
    <td><span style="color: purple;">pass</span></td>
    <td>0.95</td>
    <td>0.7</td>
    <td>0.15</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td><span style="color: purple;">fail</span></td>
    <td>0.05</td>
    <td>0.3</td>
    <td>0.85</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
  </tr>
  <tr>
    <td><span style="color: purple;">no results</span></td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>1</td>
    <td>1</td>
    <td>1</td>
  </tr>
</table>

Utility (<b><span style="color: blue;">U</span></b>) for each combination of test decision (<b><span style="color: red;">T</span></b>), buy decision (<b><span style="color: red;">B</span></b>), and oil field quality (<b><span style="color: purple;">Q</span></b>):

<table>
  <tr>
    <th style="text-align: center;"><span style="color: red;">T</span></th>
    <th style="text-align: center;"><span style="color: red;">B</span></th>
    <th style="text-align: center;"><span style="color: purple;">Q</span></th>
    <th style="text-align: center;"><span style="color: blue;">U</span></th>
  </tr>
  <tr>
    <td rowspan="6"><span style="color: red;">Perform test</span></td>
    <td rowspan="3"><span style="color: red;">Buy</span></td>
    <td><span style="color: purple;">high</span></td>
    <td><span style="color: blue;">1220</span></td>
  </tr>
  <tr>
    <td><span style="color: purple;">medium</span></td>
    <td><span style="color: blue;">600</span></td>
  </tr>
  <tr>
    <td><span style="color: purple;">low</span></td>
    <td><span style="color: blue;">-30</span></td>
  </tr>
  <tr>
    <td rowspan="3"><span style="color: red;">Do not buy</span></td>
    <td><span style="color: purple;">high</span></td>
    <td><span style="color: blue;">320</span></td>
  </tr>
  <tr>
    <td><span style="color: purple;">medium</span></td>
    <td><span style="color: blue;">320</span></td>
  </tr>
  <tr>
    <td><span style="color: purple;">low</span></td>
    <td><span style="color: blue;">320</span></td>
  </tr>
  <tr>
    <td rowspan="6"><span style="color: red;">Do not perform test</span></td>
    <td rowspan="3"><span style="color: red;">Buy</span></td>
    <td><span style="color: purple;">high</span></td>
    <td><span style="color: blue;">1250</span></td>
  </tr>
  <tr>
    <td><span style="color: purple;">medium</span></td>
    <td><span style="color: blue;">630</span></td>
  </tr>
  <tr>
    <td><span style="color: purple;">low</span></td>
    <td><span style="color: blue;">0</span></td>
  </tr>
  <tr>
    <td rowspan="3"><span style="color: red;">Do not buy</span></td>
    <td><span style="color: purple;">high</span></td>
    <td><span style="color: blue;">350</span></td>
  </tr>
  <tr>
    <td><span style="color: purple;">medium</span></td>
    <td><span style="color: blue;">350</span></td>
  </tr>
  <tr>
    <td><span style="color: purple;">low</span></td>
    <td><span style="color: blue;">350</span></td>
  </tr>
</table>

<h2 id="evaluating_oil_problem"> Evaluating the Influence Diagram </h2>

Influence diagrams were conceived as a compact, intuitive way to describe decision problems, yet practitioners still had to transform them into a different format (i.e., a decision tree) in order to evaluate them. Until the 1980s, when Shachter (1986)  showed how to evaluate the network directly, turning the diagram into a self-contained modelling and inference language.

<h3 id="evaluation_methods">Exact evaluation methods</h3>

The main methods for evaluating influence diagrams can be grouped into three families. The first family, often called **arc&nbsp;reversal&nbsp;/ node&nbsp;reduction** (Shachter, 1986), reverses arcs and sequentially eliminates variables, summing over chance nodes and maximising over decision nodes while pushing expected-utility information forward. The algorithm is elegant but intermediate graphs can become dense when nodes have many parents.

A second family extends the **variable-elimination** procedure familiar from Bayesian networks. Here we remove variables one at a time following an elimination order, multiplying factors and marginalising or maximising as required. The complexity is controlled by the size of the largest intermediate factor, which in turn depends on the diagram's treewidth, the number of decision nodes, and the width of the variable domains.

The third approach converts the diagram into a **junction tree**. Each clique holds a local sub-problem and messages are passed between cliques until convergence. Junction-tree propagation tends to be more memory-efficient than flat variable elimination and is the basis of most commercial solvers (Jensen et al., 1994). For instance, PyAgrum implements a variant of the Shafer–Shenoy message-passing algorithm—the **LIMID** algorithm (Madsen & Nilsson, 2001) implemented in `gum.ShaferShenoyLIMIDInference`—which operates on the junction tree and is extended to handle decision and utility nodes to find the Maximum Expected Utility (MEU) and optimal policies.

Evaluating influence diagrams is tractable when the diagram's treewidth stays low; then running time grows only polynomially with the number of variables. High treewidth, numerous decision nodes or wide variable domains push every exact method into exponential territory. Most real-world diagrams are built with enough structure to remain manageable, though pathological cases still exist.

<h3 id="oil_problem_solution">Worked example: solving the oil-field LIMID</h3>

The following walk-through shows every step carried out by the Shafer–Shenoy <strong>junction-tree</strong> (a.k.a. LIMID) algorithm.<br/>
Notation&nbsp;&nbsp;• chance variables: $$Q, R$$ &nbsp;• decision variables: $$T, B$$ &nbsp;• utility node: $$U$$

<h4 id="step1_moralise">1. Moralise the influence diagram</h4>

<p><em>What is moralisation and why do we need it?</em><br/>
The original influence diagram is a <strong>directed</strong> acyclic graph.  Unfortunately the Shafer–Shenoy algorithm works on an <strong>undirected</strong> structure.  We therefore transform the DAG into an undirected graph that preserves every required dependency.  The recipe is simple:
<ol style="margin-top:0.2em;margin-bottom:0.2em;">
  <li>For each child node, connect all of its parent nodes (“marry the parents”).</li>
  <li>Drop the direction of every remaining arrow.</li>
</ol>
The resulting graph is called the <strong>moral graph</strong> because parents are “married” before raising a child.  Conditional independencies encoded by the original diagram remain valid in this new undirected representation.</p>

 Connect every pair of parents of the same child and drop all arrow heads. For our diagram this adds one missing edge $$(Q,T)$$. The moral graph on decision/chance nodes is therefore the complete graph

$$
\mathcal{G}_{\text{moral}} = K_{4}\text{ on }\{T,Q,R,B\}.
$$

<h4 id="step2_triangulate">2. Triangulate (make chordal)</h4>

<p><em>Why triangulate?</em><br/>
The next step ensures that the moral graph is <strong>chordal</strong> (a cycle of four or more nodes must have a shortcut/chord).  A chordal graph has the crucial property that its maximal cliques can be arranged as a tree—the junction tree.  When the graph is already chordal, like ours, this step does nothing; otherwise extra “fill-in” edges are added until every cycle is broken.</p>

 Because the moral graph is already complete no fill-in edges are necessary under <em>any</em> elimination order. The resulting graph is chordal and the tree-width equals $$|C^{\star}|-1 = 4-1 = 3$$.

<h4 id="step3_junction_tree">3. Build the junction tree</h4>

<p><em>From cliques to a tree of cliques</em><br/>
• A <strong>clique</strong> is a fully connected subset of nodes.<br/>
• A <strong>maximal clique</strong> is a clique that cannot be enlarged by adding another neighbouring node.<br/>
<br/>
The junction-tree algorithm turns the set of maximal cliques into a tree such that for any variable, the cliques containing that variable appear <em>contiguously</em> along the tree (the <strong>running-intersection property</strong>).  Messages then need to flow only along the tree edges, never around cycles.</p>

 The unique maximal clique is $$C^{\star}=\{T,Q,R,B\}$$. For computational convenience we split it into three overlapping 3-cliques that still satisfy the running–intersection property

$$
C_1=\{T,Q,R\},\; C_2=\{T,R,B\},\; C_3=\{T,B,Q\},
$$
 with separators $$S_{12}= \{T,R\}$$ and $$S_{23}= \{T,B\}$$.

 The spanning tree $$C_1 \leftrightarrow C_2 \leftrightarrow C_3$$ is the Shafer–Shenoy junction tree.

<h4 id="step4_init">4. Attach initial valuations</h4>

<p>After the structural work is done we must populate the junction tree with <strong>numbers</strong>.  Every clique receives two tables (often called <em>potentials</em>):
<ul style="margin-top:0.2em;margin-bottom:0.2em;">
  <li>$$p_{C}$$ — a <em>probability potential</em> obtained by multiplying together all conditional probability tables whose variables are fully contained in the clique.</li>
  <li>$$u_{C}$$ — a <em>utility potential</em> constructed from the problem's utility functions.</li>
</ul>
Some cliques may receive the constant value $$1$$ for their probability part (meaning “no probability information lives here”) or $$0$$ for their utility part.  The collection of potentials is the quantitative input the message-passing algorithm will process.</p>

 Each clique $$C_i$$ carries a <em>probability potential</em> $$p_{C_i}$$ and a <em>utility potential</em> $$u_{C_i}$$:

$$
\begin{aligned}
C_1:&\; p_{C_1}(T,Q,R)=P(Q)\,P(R\mid Q,T), && u_{C_1}=0,\\
C_2:&\; p_{C_2}=1,                                && u_{C_2}=0,\\
C_3:&\; p_{C_3}=1,                                && u_{C_3}=U(T,B,Q).
\end{aligned}
$$

(The provided utility table does not depend on $$R$$, therefore the arc $$R\to U$$ carries no numbers.)

<h4 id="step5_rules">5. Shafer–Shenoy operations</h4>

<p>The Shafer–Shenoy calculus defines how these quantitative objects are manipulated during propagation.
<em>Combination</em> merges information coming from different cliques; <em>marginalisation</em> (summing or maximising) eliminates variables not needed further downstream.  Each rule below produces another pair $$(p,u)$$ so the data type remains closed under the operations.</p>

• <strong>Combine:</strong> $$(p,u)\otimes(p',u') = (pp',\,u+u')$$<br/>
• <strong>Sum out</strong> a chance variable $$X$$:
$$
\sum_{x}(p,u)=\Bigl(\sum_{x}p,\;\frac{\sum_{x}p\,u}{\sum_{x}p}\Bigr)
$$<br/>
• <strong>Optimise</strong> over a decision variable $$D$$:
$$
\max_{d}(p,u)=\bigl(\max_{d}p,\;\max_{d}u\bigr)
$$
The maximising argument is stored as the optimal policy $$\pi^{\star}_D$$.

<h4 id="step6_collect">6. Backward (collect) phase</h4>

<strong>6.1&nbsp; Message $$C_3 \rightarrow C_2$$</strong> across $$S_{23}=\{T,B\}$$. Eliminate $$Q$$:
$$
m_{3\to2}(T,B)=\bigl(1,\; \underbrace{\textstyle\sum_{Q}P(Q)\,U(T,B,Q)}_{u_{3\to2}(T,B)}\bigr).
$$

Numerically
$$
\begin{aligned}
 u_{3\to2}(\text{test},\text{buy})&=0.35\!\cdot\!1220+0.45\!\cdot\!600+0.20\!\cdot(-30)=721,\\
 u_{3\to2}(\text{test},\lnot\text{buy})&=320,\\
 u_{3\to2}(\lnot\text{test},\text{buy})&=0.35\!\cdot\!1250+0.45\!\cdot\!630+0.20\!\cdot 0=721,\\
 u_{3\to2}(\lnot\text{test},\lnot\text{buy})&=350.
\end{aligned}
$$

<strong>6.2&nbsp; Combine</strong> $$C_2 \otimes m_{3\to2}$$ and send <br/>
$$C_2 \rightarrow C_1$$ (maximise over $$B$$):
$$
 m_{2\to1}(T,R)=\max_{B}u_{3\to2}(T,B)=\begin{cases}
 721,&T=\lnot\text{test},\\
 721,&T=\text{test},\,R=\text{pass},\\
 320,&T=\text{test},\,R=\text{fail}.
 \end{cases}
$$

<strong>6.3&nbsp; Combine</strong> with $$C_1$$ and eliminate $$R$$:
$$
 EU(T=t)=\sum_{Q,R}P(Q)\,P(R\mid Q,t)\,m_{2\to1}(t,R)=\begin{cases}
 721,&t=\lnot\text{test},\\
 696.3,&t=\text{test}.
 \end{cases}
$$

<h4 id="step7_rootT">7. Choose the root decision $$T$$</h4>

The maximum expected utility (MEU) is obtained without performing the test:
$$
\boxed{T^{\star}=\lnot\text{perform test}}
$$

<p><strong>Maximum expected utility:</strong> 721&nbsp;k€.</p>

<h4 id="step8_distribute">8. Forward (distribute) phase — policy for $$B$$</h4>

With $$T^{\star}$$ fixed only two information states remain:
$$
\pi_B^{\star}(T,R)=\begin{cases}
 \text{buy},      &T=\lnot\text{test},\\
 \text{buy},      &T=\text{test},\,R=\text{pass},\\
 \lnot\text{buy}, &T=\text{test},\,R=\text{fail}.
\end{cases}
$$

<h4 id="step9_outcome">9. Outcome</h4>

The Shafer–Shenoy junction-tree algorithm therefore recommends <strong>skipping the geological test and purchasing the field immediately</strong>, yielding an MEU of 721&nbsp;k€.

All intermediate graphical steps—moralisation, triangulation, clique extraction and message passing—have been made explicit to demystify what the library function does under the hood.

<h2 id="sensitivity_analysis">Sensitivity Analysis</h2>

Actualizar los valores de utilidad tanto en este blogpost, como en el viejo, como en el código.

<!-- 
<script
	type="module"
	src="https://gradio.s3-us-west-2.amazonaws.com/5.31.0/gradio.js"
></script> 

<gradio-app src="https://ferjorosa-oil-field-purchase-decision.hf.space"></gradio-app>
-->

<h2>Conclusion</h2>

Comentar donde seguir aprendiendo mas sobre este tema y aspectos interesantes actuales como que:

es un tema poco explorado, no hemos tocado causalidad (ahi hay mas chicha), relacion con LLMs, variables probabilisticas continuas (poner referencia), etc. 

Comentar brevemente las fortalezas y limitaciones de los IDs


<h2 id="references">References</h2>

1. Shenoy, P. P. (2009). <a href="https://pshenoy.ku.edu/Papers/EOLSS09.pdf"><u>Decision trees and influence diagrams</u></a>. Encyclopedia of life support systems, 280-298.

X. Howard, R. A., Matheson, J. E. (1984). <u>Influence diagrams</u>. The Principles and Applications of Decision Analysis (Vol. II), 719-762

Jensen, F., Jensen, F. V., & Dittmer, S. (1994). From influence diagrams to junction trees. In Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence (UAI) (pp. 367-373).

Lauritzen, S. L., & Nilsson, D. (2001). Representing and solving decision problems with limited information. Management Science, 47(9), 1235-1251.

SHAFER, G., AND P. P. SHENOY. 1990. Probability Propagation. Ann. Math. Artif. Intell. 2, 327-352.
* https://kuscholarworks.ku.edu/server/api/core/bitstreams/c353aa52-11ad-46c0-b867-f5d05f7f1962/content