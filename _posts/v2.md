<h2 id="decision-trees-strengths-limitations">Strenghts and Weaknesses of Decision Trees</h2>

Decision trees provide a straightforward way to model decisions under uncertainty. Each path from start to finish shows a sequence of choices and events that lead to a specific outcome. The left-to-right layout makes the timing of decisions and chance events visually clear, helping to avoid confusion about what is known at each point. 

For small problems with only a few stages, decision trees can be evaluated using basic arithmetic. This simplicity makes them accessible to a wide audience, including those without technical training. They serve as effective tools for teaching, storytelling, and supporting real-world decisions. 

Yet this intuitive structure comes with notable drawbacks. One issue is the risk of combinatorial explosion. Another limitation is that decision trees do not explicitly show conditional independencies. Let's 

<h3 id="combinatorial-explosion" style="color:purple;">Combinatorial Explosion</h3>

The memory required to store a decision tree and the time required to process it both increase exponentially with the number of variables and their possible states, whether they are decisions or probabilistic outcomes. In a purely symmetric decision problem with $$n$$ variables, each with $$k$$ possible outcomes, you would face $$k^{n}$$ distinct combinations of outcomes or paths to evaluate. This exponential growth quickly renders large problems computationally intractable and visually unwieldy. For instance, a problem with just 50 binary variables would theoretically yield $$2^{50}$$ paths, which is utterly impractical to represent or compute [Shenoy, 2009; Russell & Norvig, 2021].

Our oil field investment scenario from <a href="/2025/06/08/decision-theory-I.html">Part I</a>, while seemingly simple, already highlights this complexity. It involves two sequential decision points (whether to test, and then whether to buy) and two sources of uncertainty (test result, and field quality). If every path in the tree were equally long and all variables were relevant in all scenarios, we might expect a maximum of $$2 \text{ (Test/No Test)} \times 2 \text{ (Buy/Not Buy)} \times 2 \text{ (Pass/Fail)} \times 3 \text{ (High/Med/Low)} = 24$$ terminal nodes. However, our problem is *asymmetric*: the "No Test" option bypasses the "Test Result" uncertainty, and the "Do Not Buy" option bypasses the "Field Quality" uncertainty. This dynamic pruning of branches results in a more manageable 12 distinct decision paths, as shown in Figure 1 [Bielza & Shenoy, 1999].

<center>
<table>
  <tr>
    <td align="center">
      <img src="/assets/2025-06-14-decision-theory-II/oil_asymmetric_tree.png" alt="Decision tree diagram of the asymmetric oil problem from Part I" width="400">
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <i><b>Figure 1.</b> Decision tree diagram of the asymmetric oil field investment problem from Part I.</i>
    </td>
  </tr>
</table>
</center>

To illustrate how quickly this can escalate, consider a slightly altered version of our oil field scenario – a *symmetric* problem. Imagine the company's options are always to either **'Invest in Field A'** or **'Invest in Field B'**. Both fields then undergo a geological test with \'Pass\' or \'Fail\' results, followed by the actual drilling which reveals \'High\', \'Medium\', or \'Low\' quality. In this symmetric structure, every path is fully explored, leading to a much larger tree:

<center>
<table>
  <tr>
    <td align="center">
      <img src="/assets/2025-06-14-decision-theory-II/oil_symmetric_tree.png" alt="Decision tree diagram of a hypothetical symmetric oil problem" width="400">
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <i><b>Figure 2.</b> Decision tree diagram of a hypothetical symmetric oil field investment problem.</i>
    </td>
  </tr>
</table>
</center>

Here, the total number of distinct decision paths would be the full $$2 \text{ (Invest A/Invest B)} \times 2 \text{ (Pass/Fail)} \times 3 \text{ (High/Med/Low)} = 12$$ for each initial investment decision. If the initial choice to \'Invest A\' or \'Invest B\' is also treated as a decision node that then branches out into these scenarios, the total tree would have $$2 \times 12 = 24$$ terminal nodes. Even with just a few variables, the tree becomes significantly more complex to draw, understand, and manage, demonstrating the core challenge of combinatorial explosion [Clemen & Reilly, 2013].

<h3 id="redundancy" style="color:purple;">Redundancy & Copy-Paste Probabilities</h3>
Shared uncertainties—think global oil price—must be replicated under every branch. Updating one probability means editing it in dozens of places, inviting inconsistencies.

<h3 id="hidden-independence" style="color:purple;">Hidden Conditional Independencies</h3>
Decision trees cannot explicitly represent conditional independencies, such as two diagnostic tests being independent given the reservoir quality. Instead, you must encode the full joint distribution, which inflates the data-elicitation effort and can obscure the true relationships between variables, leading to potential misinterpretations.

Decision trees inherently encode the full joint probability distribution, even when certain variables are conditionally independent. For example, if two diagnostic tests are independent given the underlying state of a system (e.g., reservoir quality), a decision tree still requires you to specify the probabilities for all combinations, rather than explicitly stating the independence. This inflates the data elicitation effort and obscures the underlying probabilistic relationships.

<!-- Comentar que durante años la unica manera de resolver estos problemas era con arboles de decision y hasta que en 1980 surgen los diagramas de influence, pero realmente no hubo software comun para >

<h2 id="decision-trees-strengths-limitations">Decision Trees: Strengths and Limitations</h2>

Decision trees are a crowd-pleaser. As we saw in <a href="/2025/06/08/decision-theory-I.html">Part&nbsp;I</a>, a left-to-right layout tells a self-explanatory story: *if this happens, then we choose that*. Their visual simplicity makes them ideal for workshops with non-technical stakeholders, offers an audit trail regulators love, and—when the problem is small—lets you "roll back" the expected utility with nothing more exotic than a spreadsheet.

However, those same trees can buckle once real-world complexity sneaks in. Extra decisions, shared uncertainties, feedback loops, or multi-attribute objectives quickly turn a tidy diagram into a wall-sized poster.

<h3 id="combinatorial-explosion" style="color:purple;">Combinatorial Explosion</h3>
Every additional decision point or uncertain variable rapidly increases the number of possible paths through the tree. For instance, if you add just one more binary decision and one more binary chance node, the number of potential outcomes (leaves) can quadruple. This has direct consequences for memory usage, the computational cost of operations (e.g., numerous Bayes theorem calls), and significantly impacts readability because the tree diagram becomes unmanageably large, even for moderately complex problems, making it difficult to visualize, analyze, or communicate.

<!-- Hay que escribir un ejemplo claro de la limitacion con numeros y a poder ser siguiendo el ejemplo anterior, aunque no tiene por que... -->

<h3 id="hidden-independence" style="color:purple;">Hidden Conditional Independencies</h3>
Trees cannot state explicitly that two diagnostics are independent given reservoir quality; instead you must encode the full joint distribution, inflating elicitation effort. Decision trees inherently encode the full joint probability distribution, even when certain variables are conditionally independent. For example, if two diagnostic tests are independent given the underlying state of a system (e.g., reservoir quality), a decision tree still requires you to specify the probabilities for all combinations, rather than explicitly stating the independence. This inflates the data elicitation effort and obscures the underlying probabilistic relationships.

<!-- Lo mismo para este caso, mostrar de forma clara esta limitacion -->

<div style="background-color: #e0f7fa; padding: 10px; border-radius: 5px;">
<strong>Bottom line:</strong> decision trees shine as teaching tools and for *small*, strictly sequential problems. For anything larger, <u><a href="https://en.wikipedia.org/wiki/Influence_diagram">influence diagrams</a></u> offer a compact, maintainable, and transparent alternative. The next section shows how they tame the explosion while preserving the logic.
</div>

<!-- * Comentar las limitaciones
* Hablar de que librerias en Python tenemos disponibles. Centrarme en PyAgrum y PyCID -->

