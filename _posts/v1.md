<h2 id="decision-trees-strengths-limitations">Decision Trees: Strengths and Limitations</h2>

Decision trees are a crowd-pleaser. As we saw in <a href="/2025/06/08/decision-theory-I.html">Part&nbsp;I</a>, a left-to-right layout tells a self-explanatory story: *if this happens, then we choose that*.  Their visual simplicity makes them ideal for workshops with non-technical stakeholders, offers an audit trail regulators love, and—when the problem is small—lets you "roll back" the expected utility with nothing more exotic than a spreadsheet.

However, those same trees can buckle once real-world complexity sneaks in.  Extra decisions, shared uncertainties, feedback loops, or multi-attribute objectives quickly turn a tidy diagram into a wall-sized poster.  Below we unpack the main culprits, one by one.

<h3 id="combinatorial-explosion" style="color:purple;">Combinatorial Explosion</h3>
Every additional decision point or uncertain variable rapidly increases the number of possible paths through the tree. For instance, if you add just one more binary decision and one more binary chance node, the number of potential outcomes (leaves) can quadruple. This makes the tree diagram unmanageably large, even for moderately complex problems, and difficult to visualize, analyze, or communicate.

<h3 id="redundancy" style="color:purple;">Redundancy & Copy-Paste Probabilities</h3>
Shared uncertainties—think global oil price—must be replicated under every branch. Updating one probability means editing it in dozens of places, inviting inconsistencies. When uncertainties are shared across different branches of the tree (e.g., a global economic factor impacting multiple decisions), their probabilities must be duplicated under each relevant branch. This not only makes the tree cumbersome but also introduces a significant risk of inconsistencies. If the probability of a shared uncertainty changes, you would need to update it in numerous places, which is error-prone and time-consuming.

<h3 id="hidden-independence" style="color:purple;">Hidden Conditional Independencies</h3>
Trees cannot state explicitly that two diagnostics are independent given reservoir quality; instead you must encode the full joint distribution, inflating elicitation effort. Decision trees inherently encode the full joint probability distribution, even when certain variables are conditionally independent. For example, if two diagnostic tests are independent given the underlying state of a system (e.g., reservoir quality), a decision tree still requires you to specify the probabilities for all combinations, rather than explicitly stating the independence. This inflates the data elicitation effort and obscures the underlying probabilistic relationships.

<h3 id="loops-feedback" style="color:purple;">Loops and Feedback</h3>
Dynamic problems such as price → sales → re-price require *unfolding* the tree for every time period, quickly outgrowing any reasonable page or screen. Problems involving dynamic interactions or feedback loops, where an outcome at one stage influences decisions or uncertainties in a subsequent stage (e.g., pricing strategy affecting sales, which then informs a revised pricing strategy), are difficult to represent directly in a standard decision tree. These situations require "unfolding" the tree over multiple time periods, leading to an even more rapid growth in complexity and size, often making them impractical to model.

<h3 id="what-if-pain" style="color:purple;">What-If Analysis Pain</h3>
Sensitivity questions—"What if porosity-test accuracy drops from 95&nbsp;% to 80&amp;nbsp;%?"—force you to rebuild and re-solve the entire tree. Performing sensitivity analysis—asking "what if" questions about changes in probabilities, values, or decision sequences—is cumbersome with decision trees. Each "what if" scenario often necessitates rebuilding and re-solving a significant portion or even the entirety of the tree. This makes exploring a range of assumptions or testing the robustness of decisions a highly laborious process.

<h3 id="multi-attribute" style="color:purple;">Multi-Attribute Utilities Buried at the Leaves</h3>
Environmental impact, cash flow, and reputation must be baked into a single number at each terminal node, obscuring the contribution of each attribute. In real-world problems, outcomes often have multiple dimensions (e.g., financial profit, environmental impact, reputation, safety). Decision trees typically require these multiple attributes to be combined into a single utility value at each terminal node. This aggregation can obscure the individual contributions of each attribute, making it difficult to understand trade-offs or communicate the multi-faceted implications of different decisions.

<h3 id="multi-agent" style="color:purple;">Scaling to Multiple Agents</h3>
Add a competitor's move and the model becomes a *game tree*, doubling complexity and introducing fiddly information-set notation. When a problem involves strategic interactions between multiple decision-makers (e.g., a company and its competitor), the model transforms into a game tree. This introduces an entirely new layer of complexity, often requiring specialized solution concepts (like Nash equilibrium) and notation for information sets, which further doubles the complexity and makes the analysis considerably more intricate.

<h3 id="readability-ceiling" style="color:purple;">Readability Ceiling</h3>
Empirically, stakeholders stop digesting trees once they exceed about 30 nodes; beyond that, the diagram becomes a poster nobody wants to maintain. Despite their initial visual appeal, decision trees quickly hit a "readability ceiling." Empirical observations suggest that stakeholders typically find trees difficult to digest once they exceed approximately 30 nodes. Beyond this point, the diagram becomes a sprawling "poster" that is hard to follow, challenging to maintain, and often fails to effectively communicate the decision logic to non-technical audiences.

<div style="background-color: #e0f7fa; padding: 10px; border-radius: 5px;">
<strong>Bottom line:</strong> decision trees shine as teaching tools and for *small*, strictly sequential problems.  For anything larger, <u><a href="https://en.wikipedia.org/wiki/Influence_diagram">influence diagrams</a></u> offer a compact, maintainable, and transparent alternative.  The next section shows how they tame the explosion while preserving the logic.
</div>

<!-- * Comentar las limitaciones 
* Hablar de que librerias en Python tenemos disponibles. Centrarme en PyAgrum y PyCID -->