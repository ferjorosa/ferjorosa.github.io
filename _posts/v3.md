
<h2 id="decision-trees-strengths-limitations">Strenghts and Weaknesses of Decision Trees</h2>

Decision trees provide a straightforward way to model decisions under uncertainty. Each path from start to finish shows a sequence of choices and events that lead to a specific outcome. The left-to-right layout makes the timing of decisions and chance events visually clear, helping to avoid confusion about what is known at each point. 

For small problems with only a few stages, decision trees can be evaluated using basic arithmetic. This simplicity makes them accessible to a wide audience, including those without technical training. They serve as effective tools for teaching, storytelling, and supporting real-world decisions. 

Yet this intuitive structure comes with notable drawbacks. One issue is the risk of combinatorial explosion. Another limitation is that decision trees do not explicitly show conditional independencies. Let's 

<h3 id="combinatorial-explosion">Combinatorial Explosion</h3>
The memory required to store a decision tree and the time required to process it both **increase
exponentially** with the number of variables and their possible states, whether they are decisions or
probabilistic outcomes. In a symmetric problem with $$n$$ variables, each having $$k$$ possible outcomes, you face $$k^{n}$$ distinct paths. Since a decision tree represents all scenarios explicitly, a problem with 50 binary variables would yield an impractical $$2^{50}$$ paths ([Shenoy, 2009](https://pshenoy.ku.edu/Papers/EOLSS09.pdf)).

The number of decision paths is profoundly affected by the order and meaning of the variables (i.e., the problem's definition). In our original oil field investment problem from <a href="https://ferjorosa.github.io/blog/2025/06/08/decision-theory-I.html">Part I</a>, the options to "Not Test" or "Do Not Buy" prune the tree, resulting in 12 distinct decision paths. This is an *asymmetric* problem structure:

<center>
<table>
  <tr>
    <td align="center">
      <img src="/assets/2025-06-14-decision-theory-II/oil_asymmetric_tree.png" alt="Decision tree diagram of the asymmetric oil problem from Part I" height="200">
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <i><b>Figure 1.</b> Decision tree diagram of the asymmetric oil field investment problem from Part I.</i>
    </td>
  </tr>
</table>
</center>

Conversely, a problem with the same types of variables, but structured *symmetrically*, would yield significantly more paths. For instance, imagine the company must always choose to either "Invest in Field A" or "Invest in Field B" (instead of "Buy" or "Do Not Buy"). Both fields then undergo a geological test (say, "Test X" or "Test Y", each with different costs and accuracy profiles), followed by the actual drilling revealing "High", "Medium", or "Low" quality. This structure forces every path to be fully explored, doubling the number of distinct paths from 12 to 24:

<center>
<table>
  <tr>
    <td align="center">
      <img src="/assets/2025-06-14-decision-theory-II/oil_symmetric_tree.png" alt="Decision tree diagram of a hypothetical symmetric oil problem" height="200">
    </td>
  </tr>
  <tr>
    <td colspan="2" align="center">
      <i><b>Figure 2.</b> Decision tree diagram of a hypothetical symmetric oil field investment problem.</i>
    </td>
  </tr>
</table>
</center>

Even this revised example demonstrates how swiftly a decision tree can escalate beyond practical use. For instance, merely replacing the three-level oil quality (high/medium/low) with a more granular five-level scale (excellent/good/average/poor/dry) would push the total from 24 to 40 terminal nodes. This occurs without even considering longer time horizons, dynamic market-price scenarios, or additional complex choices.

This **combinatorial explosion** not only affects computational tractability but, even at more modest levels, severely compromises interpretability. As trees grow larger (e.g., once they reach around a hundred terminal nodes), they lose their key strength: easy readability and intuitive understanding.


<h3 id="hidden-independence">Hidden Conditional Independencies</h3>

Decision trees, by their very structure, imply a strict chain of dependence. Every variable in a given branch is implicitly assumed to be conditioned on *all* preceding events on that path. This makes it hard—sometimes impossible—to state that two uncertainties are **independent once a third variable is known**.

Consider a more concrete oil-field scenario. Suppose the company can run **two different exploration tests** before drilling:

1.  **Porosity test (P)** – returns *Pass* (porosity ≥ 15%) or *Fail* (porosity < 15%).
2.  **Seismic survey (S)** – returns *Good*, *Average*, or *Poor* signal quality.

Both tests are driven by the *true reservoir quality* **(Q)**, which can be *High*, *Medium*, or *Low*. Critically, once you know Q, the results of the two tests do **not influence each other**. In probability language:

$$\Pr(P, S \mid Q) = \Pr(P \mid Q)\,\Pr(S \mid Q).$$

That single line expresses the conditional independence of *P* and *S* given *Q*.

In a decision tree, however, you cannot state this independence explicitly. You must specify the *joint* distribution of the two test outcomes for every quality level: $$\Pr(P{=}p, S{=}s \mid Q{=}q)$$ for all 2 outcomes for P × 3 outcomes for S × 3 quality levels = 18 combinations. This means you are filling in extra cells in the probability table that, if conditional independencies were considered, would not be needed. If each test had five possible outcomes, the table would balloon to 75 entries. The larger the problem, the faster the data-elicitation burden explodes.

Influence diagrams solve this elegantly. You draw arrows from Q to P and from Q to S, but no arrow between P and S, making the conditional independence visually explicit. The conditional-probability tables shrink to just the five rows needed to specify $$\Pr(P \mid Q)$$ (2 rows) and $$\Pr(S \mid Q)$$ (3 rows).

<center>
<em>Proposed figure:</em> side-by-side comparison of (a) a fragment of the decision tree showing the porosity test and seismic survey nested one after the other, and (b) an influence diagram with Q pointing separately to P and S, highlighting the missing arc between P and S.
</center>

Not only does this cut the parameter count, it also keeps the model transparent and easy to maintain. If new geological evidence refines the distribution of Q, you update two small tables rather than a cumbersome joint table.

This becomes clearer with a small numerical example.  Assume the following probabilities when the reservoir quality is **High**:

|               | S = Good | S = Average | S = Poor |
|---------------|:--------:|:-----------:|:--------:|
| **P = Pass**  | 0.72     | 0.135       | 0.045    |
| **P = Fail**  | 0.08     | 0.015       | 0.005    |

All six cells must be entered (and the total replicated for **Medium** and **Low** quality as well), giving **18 numbers** in total.

Under the independence captured by an influence diagram, you only need to elicit the two much smaller tables below:

*Porosity test conditional on Q*

| Q            | Pass | Fail |
|--------------|:----:|:----:|
| High         | 0.90 | 0.10 |
| Medium       | 0.60 | 0.40 |
| Low          | 0.20 | 0.80 |

*Seismic survey conditional on Q*

| Q            | Good | Average | Poor |
|--------------|:----:|:-------:|:----:|
| High         | 0.80 | 0.15    | 0.05 |
| Medium       | 0.50 | 0.30    | 0.20 |
| Low          | 0.20 | 0.30    | 0.50 |

That is **3 fewer entries** than the joint-table approach, while conveying the same information more transparently.

Below is the **complete 18-cell joint table** that a decision-tree approach would require (numbers are simply the products of the two conditional tables above, but they would have to be entered explicitly if independence were not recognised):

| Q      | P  | S = Good | S = Average | S = Poor |
|--------|----|:--------:|:-----------:|:--------:|
| High   | Pass | 0.72 | 0.135 | 0.045 |
| High   | Fail | 0.08 | 0.015 | 0.005 |
| Medium | Pass | 0.30 | 0.18  | 0.12  |
| Medium | Fail | 0.20 | 0.12  | 0.08  |
| Low    | Pass | 0.04 | 0.06  | 0.10  |
| Low    | Fail | 0.16 | 0.24  | 0.40  |

Even for this toy problem you can see how quickly the table grows. Imagine adding just one more test—and then another quality level. The joint table would explode to 36, then 48 entries, while the influence-diagram representation would still scale linearly.

<h2 id="references">References</h2>

1. Shenoy, P. P. (2009). <a href="https://pshenoy.ku.edu/Papers/EOLSS09.pdf"><u>Decision trees and influence diagrams</u></a>. Encyclopedia of life support systems, 280-298.